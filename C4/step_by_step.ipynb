{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPUbMH7kWkaz7fgRMiHb9aj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"vzAdTB73Qtr7","executionInfo":{"status":"ok","timestamp":1703680370485,"user_tz":-390,"elapsed":7099,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}}},"outputs":[],"source":["import math\n","import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow.python.framework import ops\n","\n","\n","def load_dataset():\n","    train_dataset = h5py.File('/content/train_signs.h5', \"r\")\n","    # your train set features\n","    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n","    train_set_y_orig = np.array(\n","        train_dataset[\"train_set_y\"][:])  # your train set labels\n","\n","    test_dataset = h5py.File('/content/test_signs.h5', \"r\")\n","    # your test set features\n","    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])\n","    test_set_y_orig = np.array(\n","        test_dataset[\"test_set_y\"][:])  # your test set labels\n","\n","    classes = np.array(test_dataset[\"list_classes\"][:])  # the list of classes\n","\n","    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n","    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n","\n","    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n","\n","\n","def random_mini_batches(X, Y, mini_batch_size=64, seed=0):\n","    \"\"\"\n","    Creates a list of random minibatches from (X, Y)\n","\n","    Arguments:\n","    X -- input data, of shape (input size, number of examples) (m, Hi, Wi, Ci)\n","    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) (m, n_y)\n","    mini_batch_size - size of the mini-batches, integer\n","    seed -- this is only for the purpose of grading, so that you're \"random minibatches are the same as ours.\n","\n","    Returns:\n","    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n","    \"\"\"\n","\n","    m = X.shape[0]                  # number of training examples\n","    mini_batches = []\n","    np.random.seed(seed)\n","\n","    # Step 1: Shuffle (X, Y)\n","    permutation = list(np.random.permutation(m))\n","    shuffled_X = X[permutation, :, :, :]\n","    shuffled_Y = Y[permutation, :]\n","\n","    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n","    # number of mini batches of size mini_batch_size in your partitionning\n","    num_complete_minibatches = math.floor(m / mini_batch_size)\n","    for k in range(0, num_complete_minibatches):\n","        mini_batch_X = shuffled_X[k * mini_batch_size: k *\n","                                  mini_batch_size + mini_batch_size, :, :, :]\n","        mini_batch_Y = shuffled_Y[k * mini_batch_size: k *\n","                                  mini_batch_size + mini_batch_size, :]\n","        mini_batch = (mini_batch_X, mini_batch_Y)\n","        mini_batches.append(mini_batch)\n","\n","    # Handling the end case (last mini-batch < mini_batch_size)\n","    if m % mini_batch_size != 0:\n","        mini_batch_X = shuffled_X[num_complete_minibatches *\n","                                  mini_batch_size: m, :, :, :]\n","        mini_batch_Y = shuffled_Y[num_complete_minibatches *\n","                                  mini_batch_size: m, :]\n","        mini_batch = (mini_batch_X, mini_batch_Y)\n","        mini_batches.append(mini_batch)\n","\n","    return mini_batches\n","\n","\n","def convert_to_one_hot(Y, C):\n","    Y = np.eye(C)[Y.reshape(-1)].T\n","    return Y\n","\n","\n","def forward_propagation_for_predict(X, parameters):\n","    \"\"\"\n","    Implements the forward propagation for the model: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n","\n","    Arguments:\n","    X -- input dataset placeholder, of shape (input size, number of examples)\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n","                  the shapes are given in initialize_parameters\n","\n","    Returns:\n","    Z3 -- the output of the last LINEAR unit\n","    \"\"\"\n","\n","    # Retrieve the parameters from the dictionary \"parameters\"\n","    W1 = parameters['W1']\n","    b1 = parameters['b1']\n","    W2 = parameters['W2']\n","    b2 = parameters['b2']\n","    W3 = parameters['W3']\n","    b3 = parameters['b3']\n","    # Numpy Equivalents:\n","    # Z1 = np.dot(W1, X) + b1\n","    Z1 = tf.add(tf.matmul(W1, X), b1)\n","    A1 = tf.nn.relu(Z1)                                    # A1 = relu(Z1)\n","    # Z2 = np.dot(W2, a1) + b2\n","    Z2 = tf.add(tf.matmul(W2, A1), b2)\n","    A2 = tf.nn.relu(Z2)                                    # A2 = relu(Z2)\n","    # Z3 = np.dot(W3,Z2) + b3\n","    Z3 = tf.add(tf.matmul(W3, A2), b3)\n","\n","    return Z3\n","\n","\n","def predict(X, parameters):\n","\n","    W1 = tf.convert_to_tensor(parameters[\"W1\"])\n","    b1 = tf.convert_to_tensor(parameters[\"b1\"])\n","    W2 = tf.convert_to_tensor(parameters[\"W2\"])\n","    b2 = tf.convert_to_tensor(parameters[\"b2\"])\n","    W3 = tf.convert_to_tensor(parameters[\"W3\"])\n","    b3 = tf.convert_to_tensor(parameters[\"b3\"])\n","\n","    params = {\"W1\": W1,\n","              \"b1\": b1,\n","              \"W2\": W2,\n","              \"b2\": b2,\n","              \"W3\": W3,\n","              \"b3\": b3}\n","\n","    x = tf.placeholder(\"float\", [12288, 1])\n","\n","    z3 = forward_propagation_for_predict(x, params)\n","    p = tf.argmax(z3)\n","\n","    sess = tf.Session()\n","    prediction = sess.run(p, feed_dict={x: X})\n","\n","    return prediction\n"]},{"cell_type":"code","source":["import numpy as np\n","import h5py\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","np.random.seed(1)"],"metadata":{"id":"8nzOCMaCQwqu","executionInfo":{"status":"ok","timestamp":1703680433765,"user_tz":-390,"elapsed":635,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# GRADED FUNCTION: zero_pad\n","\n","def zero_pad(X, pad):\n","    \"\"\"\n","    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image,\n","    as illustrated in Figure 1.\n","\n","    Argument:\n","    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n","    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n","\n","    Returns:\n","    X_pad -- padded image of shape (m, n_H + 2 * pad, n_W + 2 * pad, n_C)\n","    \"\"\"\n","\n","    #(≈ 1 line)\n","    # X_pad = None\n","    # YOUR CODE STARTS HERE\n","    X_pad = np.pad(X,((0,0), (pad,pad), (pad,pad), (0,0)), mode='constant', constant_values = (0,0))\n","\n","    # YOUR CODE ENDS HERE\n","\n","    return X_pad"],"metadata":{"id":"eJx3TUURREEP","executionInfo":{"status":"ok","timestamp":1703680444732,"user_tz":-390,"elapsed":569,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# GRADED FUNCTION: conv_single_step\n","\n","def conv_single_step(a_slice_prev, W, b):\n","    \"\"\"\n","    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation\n","    of the previous layer.\n","\n","    Arguments:\n","    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n","    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n","    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n","\n","    Returns:\n","    Z -- a scalar value, the result of convolving the sliding window (W, b) on a slice x of the input data\n","    \"\"\"\n","\n","    #(≈ 3 lines of code)\n","    # Element-wise product between a_slice_prev and W. Do not add the bias yet.\n","    # s = None\n","    # Sum over all entries of the volume s.\n","    # Z = None\n","    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n","    # Z = None\n","    # YOUR CODE STARTS HERE\n","    s = a_slice_prev * W\n","    Z = np.sum(s)\n","    Z = Z + np.float(b)\n","\n","    # YOUR CODE ENDS HERE\n","\n","    return Z"],"metadata":{"id":"aXOaINduRISg","executionInfo":{"status":"ok","timestamp":1703680458094,"user_tz":-390,"elapsed":562,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# GRADED FUNCTION: conv_forward\n","\n","def conv_forward(A_prev, W, b, hparameters):\n","    \"\"\"\n","    Implements the forward propagation for a convolution function\n","\n","    Arguments:\n","    A_prev -- output activations of the previous layer,\n","        numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n","    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n","    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n","    hparameters -- python dictionary containing \"stride\" and \"pad\"\n","\n","    Returns:\n","    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n","    cache -- cache of values needed for the conv_backward() function\n","    \"\"\"\n","\n","    # Retrieve dimensions from A_prev's shape (≈1 line)\n","    # (m, n_H_prev, n_W_prev, n_C_prev) = None\n","\n","    # Retrieve dimensions from W's shape (≈1 line)\n","    # (f, f, n_C_prev, n_C) = None\n","\n","    # Retrieve information from \"hparameters\" (≈2 lines)\n","    # stride = None\n","    # pad = None\n","\n","    # Compute the dimensions of the CONV output volume using the formula given above.\n","    # Hint: use int() to apply the 'floor' operation. (≈2 lines)\n","    # n_H = None\n","    # n_W = None\n","\n","    # Initialize the output volume Z with zeros. (≈1 line)\n","    # Z = None\n","\n","    # Create A_prev_pad by padding A_prev\n","    # A_prev_pad = None\n","\n","    # for i in range(None):               # loop over the batch of training examples\n","        # a_prev_pad = None               # Select ith training example's padded activation\n","        # for h in range(None):           # loop over vertical axis of the output volume\n","            # Find the vertical start and end of the current \"slice\" (≈2 lines)\n","            # vert_start = None\n","            # vert_end = None\n","\n","            # for w in range(None):       # loop over horizontal axis of the output volume\n","                # Find the horizontal start and end of the current \"slice\" (≈2 lines)\n","                # horiz_start = None\n","                # horiz_end = None\n","\n","                # for c in range(None):   # loop over channels (= #filters) of the output volume\n","\n","                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n","                    # a_slice_prev = None\n","\n","                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈3 line)\n","                    # weights = None\n","                    # biases = None\n","                    # Z[i, h, w, c] = None\n","    # YOUR CODE STARTS HERE\n","    # Retrieve dimensions from A_prev's shape (≈1 line)\n","    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n","\n","    # Retrieve dimensions from W's shape (≈1 line)\n","    (f, f, n_C_prev, n_C) = W.shape\n","\n","    # Retrieve information from \"hparameters\" (≈2 lines)\n","    stride = hparameters[\"stride\"]\n","    pad = hparameters[\"pad\"]\n","\n","    # Compute the dimensions of the CONV output volume using the formula given above.\n","    # Hint: use int() to apply the 'floor' operation. (≈2 lines)\n","    n_H = int((n_H_prev -f + 2*pad)/stride) + 1\n","    n_W = int((n_W_prev -f + 2*pad)/stride) + 1\n","\n","    # Initialize the output volume Z with zeros. (≈1 line)\n","    Z = np.zeros((m,n_H,n_W,n_C))\n","\n","    # Create A_prev_pad by padding A_prev\n","    A_prev_pad = zero_pad(A_prev, pad)\n","\n","    for i in range(m):               # loop over the batch of training examples\n","        a_prev_pad = A_prev_pad[i]               # Select ith training example's padded activation\n","        for h in range(n_H):           # loop over vertical axis of the output volume\n","            # Find the vertical start and end of the current \"slice\" (≈2 lines)\n","            vert_start = h * stride\n","            vert_end = vert_start + f\n","\n","            for w in range(n_W):       # loop over horizontal axis of the output volume\n","                # Find the horizontal start and end of the current \"slice\" (≈2 lines)\n","                horiz_start = w * stride\n","                horiz_end = horiz_start + f\n","\n","                for c in range(n_C):   # loop over channels (= #filters) of the output volume\n","\n","                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n","                    a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n","\n","                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈3 line)\n","                    weights = W[:, :, :, c]\n","                    biases = b[:, :, :, c]\n","                    Z[i, h, w, c] = conv_single_step(a_slice_prev, weights, biases)\n","\n","\n","\n","    # YOUR CODE ENDS HERE\n","\n","    # Save information in \"cache\" for the backprop\n","    cache = (A_prev, W, b, hparameters)\n","\n","    return Z, cache"],"metadata":{"id":"x5zdcmjPRLkW","executionInfo":{"status":"ok","timestamp":1703680472468,"user_tz":-390,"elapsed":606,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# GRADED FUNCTION: pool_forward\n","\n","def pool_forward(A_prev, hparameters, mode = \"max\"):\n","    \"\"\"\n","    Implements the forward pass of the pooling layer\n","\n","    Arguments:\n","    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n","    hparameters -- python dictionary containing \"f\" and \"stride\"\n","    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n","\n","    Returns:\n","    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n","    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters\n","    \"\"\"\n","\n","    # Retrieve dimensions from the input shape\n","    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n","\n","    # Retrieve hyperparameters from \"hparameters\"\n","    f = hparameters[\"f\"]\n","    stride = hparameters[\"stride\"]\n","\n","    # Define the dimensions of the output\n","    n_H = int(1 + (n_H_prev - f) / stride)\n","    n_W = int(1 + (n_W_prev - f) / stride)\n","    n_C = n_C_prev\n","\n","    # Initialize output matrix A\n","    A = np.zeros((m, n_H, n_W, n_C))\n","\n","    # for i in range(None):                         # loop over the training examples\n","        # for h in range(None):                     # loop on the vertical axis of the output volume\n","            # Find the vertical start and end of the current \"slice\" (≈2 lines)\n","            # vert_start = None\n","            # vert_end = None\n","\n","            # for w in range(None):                 # loop on the horizontal axis of the output volume\n","                # Find the vertical start and end of the current \"slice\" (≈2 lines)\n","                # horiz_start = None\n","                # horiz_end = None\n","\n","                # for c in range (None):            # loop over the channels of the output volume\n","\n","                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n","                    # a_prev_slice = None\n","\n","                    # Compute the pooling operation on the slice.\n","                    # Use an if statement to differentiate the modes.\n","                    # Use np.max and np.mean.\n","                    # if mode == \"max\":\n","                        # A[i, h, w, c] = None\n","                    # elif mode == \"average\":\n","                        # A[i, h, w, c] = None\n","\n","    # YOUR CODE STARTS HERE\n","    for i in range(m):                         # loop over the training examples\n","\n","        for h in range(n_H):                     # loop on the vertical axis of the output volume\n","            # Find the vertical start and end of the current \"slice\" (≈2 lines)\n","            vert_start = h * stride\n","            vert_end = vert_start + f\n","\n","            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n","                # Find the vertical start and end of the current \"slice\" (≈2 lines)\n","                horiz_start = w * stride\n","                horiz_end = horiz_start + f\n","\n","                for c in range (n_C):            # loop over the channels of the output volume\n","\n","                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n","                    a_prev_slice = A_prev[i,vert_start:vert_end,horiz_start:horiz_end,c]\n","\n","                    # Compute the pooling operation on the slice.\n","                    # Use an if statement to differentiate the modes.\n","                    # Use np.max and np.mean.\n","                    if mode == \"max\":\n","                        A[i, h, w, c] = np.max(a_prev_slice)\n","                    elif mode == \"average\":\n","                        A[i, h, w, c] = np.mean(a_prev_slice)\n","\n","    # YOUR CODE ENDS HERE\n","\n","    # Store the input and hparameters in \"cache\" for pool_backward()\n","    cache = (A_prev, hparameters)\n","\n","    # Making sure your output shape is correct\n","    assert(A.shape == (m, n_H, n_W, n_C))\n","\n","    return A, cache"],"metadata":{"id":"RjCny82XRPDY","executionInfo":{"status":"ok","timestamp":1703680485586,"user_tz":-390,"elapsed":344,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"a9eeYATpRSVK"},"execution_count":null,"outputs":[]}]}