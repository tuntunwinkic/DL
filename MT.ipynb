{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMyAV2H+1E3+Os4iknn4t4Q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install faker"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"35bbzj4xnVzk","executionInfo":{"status":"ok","timestamp":1714037826269,"user_tz":-390,"elapsed":10885,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}},"outputId":"503d23cb-ac2a-4659-a1c0-170c3c0ec64b"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting faker\n","  Downloading Faker-24.11.0-py3-none-any.whl (1.8 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.8 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m1.6/1.8 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n","Installing collected packages: faker\n","Successfully installed faker-24.11.0\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lZVbz6xprcjO","executionInfo":{"status":"ok","timestamp":1714038906016,"user_tz":-390,"elapsed":19146,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}},"outputId":"06ae59f5-d130-4984-d21f-898eed6bb677"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ltV3zXI3nMnI","executionInfo":{"status":"ok","timestamp":1714037835052,"user_tz":-390,"elapsed":5005,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}}},"outputs":[],"source":["import numpy as np\n","from faker import Faker\n","import random\n","from tqdm import tqdm\n","from babel.dates import format_date\n","from tensorflow.keras.utils import to_categorical\n","import tensorflow.keras.backend as K\n","from tensorflow.keras.models import Model\n","import matplotlib.pyplot as plt\n","\n","fake = Faker()\n","Faker.seed(12345)\n","random.seed(12345)\n","\n","# Define format of the data we would like to generate\n","FORMATS = ['short',\n","           'medium',\n","           'long',\n","           'full',\n","           'full',\n","           'full',\n","           'full',\n","           'full',\n","           'full',\n","           'full',\n","           'full',\n","           'full',\n","           'full',\n","           'd MMM YYY',\n","           'd MMMM YYY',\n","           'dd MMM YYY',\n","           'd MMM, YYY',\n","           'd MMMM, YYY',\n","           'dd, MMM YYY',\n","           'd MM YY',\n","           'd MMMM YYY',\n","           'MMMM d YYY',\n","           'MMMM d, YYY',\n","           'dd.MM.YY']\n","\n","# change this if you want it to work with another language\n","LOCALES = ['en_US']\n","\n","def load_date():\n","    \"\"\"\n","        Loads some fake dates\n","        :returns: tuple containing human readable string, machine readable string, and date object\n","    \"\"\"\n","    dt = fake.date_object()\n","\n","    try:\n","        human_readable = format_date(dt, format=random.choice(FORMATS),  locale='en_US') # locale=random.choice(LOCALES))\n","        human_readable = human_readable.lower()\n","        human_readable = human_readable.replace(',','')\n","        machine_readable = dt.isoformat()\n","\n","    except AttributeError as e:\n","        return None, None, None\n","\n","    return human_readable, machine_readable, dt\n","\n","def load_dataset(m):\n","    \"\"\"\n","        Loads a dataset with m examples and vocabularies\n","        :m: the number of examples to generate\n","    \"\"\"\n","\n","    human_vocab = set()\n","    machine_vocab = set()\n","    dataset = []\n","    Tx = 30\n","\n","\n","    for i in tqdm(range(m)):\n","        h, m, _ = load_date()\n","        if h is not None:\n","            dataset.append((h, m))\n","            human_vocab.update(tuple(h))\n","            machine_vocab.update(tuple(m))\n","\n","    human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'],\n","                     list(range(len(human_vocab) + 2))))\n","    inv_machine = dict(enumerate(sorted(machine_vocab)))\n","    machine = {v:k for k,v in inv_machine.items()}\n","\n","    return dataset, human, machine, inv_machine\n","\n","def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n","\n","    X, Y = zip(*dataset)\n","\n","    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n","    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n","\n","    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n","    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n","\n","\n","\n","    return X, np.array(Y), Xoh, Yoh\n","\n","def string_to_int(string, length, vocab):\n","    \"\"\"\n","    Converts all strings in the vocabulary into a list of integers representing the positions of the\n","    input string's characters in the \"vocab\"\n","\n","    Arguments:\n","    string -- input string, e.g. 'Wed 10 Jul 2007'\n","    length -- the number of time steps you'd like, determines if the output will be padded or cut\n","    vocab -- vocabulary, dictionary used to index every character of your \"string\"\n","\n","    Returns:\n","    rep -- list of integers (or '<unk>') (size = length) representing the position of the string's character in the vocabulary\n","    \"\"\"\n","\n","    #make lower to standardize\n","    string = string.lower()\n","    string = string.replace(',','')\n","\n","    if len(string) > length:\n","        string = string[:length]\n","\n","    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\n","\n","    if len(string) < length:\n","        rep += [vocab['<pad>']] * (length - len(string))\n","\n","    #print (rep)\n","    return rep\n","\n","\n","def int_to_string(ints, inv_vocab):\n","    \"\"\"\n","    Output a machine readable list of characters based on a list of indexes in the machine's vocabulary\n","\n","    Arguments:\n","    ints -- list of integers representing indexes in the machine's vocabulary\n","    inv_vocab -- dictionary mapping machine readable indexes to machine readable characters\n","\n","    Returns:\n","    l -- list of characters corresponding to the indexes of ints thanks to the inv_vocab mapping\n","    \"\"\"\n","\n","    l = [inv_vocab[i] for i in ints]\n","    return l\n","\n","\n","EXAMPLES = ['3 May 1979', '5 Apr 09', '20th February 2016', 'Wed 10 Jul 2007']\n","\n","def run_example(model, input_vocabulary, inv_output_vocabulary, text):\n","    encoded = string_to_int(text, TIME_STEPS, input_vocabulary)\n","    prediction = model.predict(np.array([encoded]))\n","    prediction = np.argmax(prediction[0], axis=-1)\n","    return int_to_string(prediction, inv_output_vocabulary)\n","\n","def run_examples(model, input_vocabulary, inv_output_vocabulary, examples=EXAMPLES):\n","    predicted = []\n","    for example in examples:\n","        predicted.append(''.join(run_example(model, input_vocabulary, inv_output_vocabulary, example)))\n","        print('input:', example)\n","        print('output:', predicted[-1])\n","    return predicted\n","\n","\n","def softmax(x, axis=1):\n","    \"\"\"Softmax activation function.\n","    # Arguments\n","        x : Tensor.\n","        axis: Integer, axis along which the softmax normalization is applied.\n","    # Returns\n","        Tensor, output of softmax transformation.\n","    # Raises\n","        ValueError: In case `dim(x) == 1`.\n","    \"\"\"\n","    ndim = K.ndim(x)\n","    if ndim == 2:\n","        return K.softmax(x)\n","    elif ndim > 2:\n","        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n","        s = K.sum(e, axis=axis, keepdims=True)\n","        return e / s\n","    else:\n","        raise ValueError('Cannot apply softmax to a tensor that is 1D')\n","\n","\n","def plot_attention_map(modelx, input_vocabulary, inv_output_vocabulary, text, n_s = 128, num = 7):\n","    \"\"\"\n","    Plot the attention map.\n","\n","    \"\"\"\n","    attention_map = np.zeros((10, 30))\n","    layer = modelx.get_layer('attention_weights')\n","\n","    Ty, Tx = attention_map.shape\n","\n","    human_vocab_size = 37\n","\n","    # Well, this is cumbersome but this version of tensorflow-keras has a bug that affects the\n","    # reuse of layers in a model with the functional API.\n","    # So, I have to recreate the model based on the functional\n","    # components and connect then one by one.\n","    # ideally it can be done simply like this:\n","    # layer = modelx.layers[num]\n","    # f = Model(modelx.inputs, [layer.get_output_at(t) for t in range(Ty)])\n","    #\n","\n","    X = modelx.inputs[0]\n","    s0 = modelx.inputs[1]\n","    c0 = modelx.inputs[2]\n","    s = s0\n","    c = s0\n","\n","    a = modelx.layers[2](X)\n","    outputs = []\n","\n","    for t in range(Ty):\n","        s_prev = s\n","        s_prev = modelx.layers[3](s_prev)\n","        concat = modelx.layers[4]([a, s_prev])\n","        e = modelx.layers[5](concat)\n","        energies = modelx.layers[6](e)\n","        alphas = modelx.layers[7](energies)\n","        context = modelx.layers[8]([alphas, a])\n","        # Don't forget to pass: initial_state = [hidden state, cell state] (≈ 1 line)\n","        s, _, c = modelx.layers[10](context, initial_state = [s, c])\n","        outputs.append(energies)\n","\n","    f = Model(inputs=[X, s0, c0], outputs = outputs)\n","\n","\n","    s0 = np.zeros((1, n_s))\n","    c0 = np.zeros((1, n_s))\n","    encoded = np.array(string_to_int(text, Tx, input_vocabulary)).reshape((1, 30))\n","    encoded = np.array(list(map(lambda x: to_categorical(x, num_classes=len(input_vocabulary)), encoded)))\n","\n","\n","    r = f([encoded, s0, c0])\n","\n","    for t in range(Ty):\n","        for t_prime in range(Tx):\n","            attention_map[t][t_prime] = r[t][0, t_prime]\n","\n","    # Normalize attention map\n","    row_max = attention_map.max(axis=1)\n","    attention_map = attention_map / row_max[:, None]\n","\n","    prediction = modelx.predict([encoded, s0, c0])\n","\n","    predicted_text = []\n","    for i in range(len(prediction)):\n","        predicted_text.append(int(np.argmax(prediction[i], axis=1)))\n","\n","    predicted_text = list(predicted_text)\n","    predicted_text = int_to_string(predicted_text, inv_output_vocabulary)\n","    text_ = list(text)\n","\n","    # get the lengths of the string\n","    input_length = len(text)\n","    output_length = Ty\n","\n","    # Plot the attention_map\n","    plt.clf()\n","    f = plt.figure(figsize=(8, 8.5))\n","    ax = f.add_subplot(1, 1, 1)\n","\n","    # add image\n","    i = ax.imshow(attention_map, interpolation='nearest', cmap='Blues')\n","\n","    # add colorbar\n","    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])\n","    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n","    cbar.ax.set_xlabel('Alpha value (Probability output of the \"softmax\")', labelpad=2)\n","\n","    # add labels\n","    ax.set_yticks(range(output_length))\n","    ax.set_yticklabels(predicted_text[:output_length])\n","\n","    ax.set_xticks(range(input_length))\n","    ax.set_xticklabels(text_[:input_length], rotation=45)\n","\n","    ax.set_xlabel('Input Sequence')\n","    ax.set_ylabel('Output Sequence')\n","\n","    # add grid and legend\n","    ax.grid()\n","\n","    #f.show()\n","\n","    return attention_map\n"]},{"cell_type":"code","source":["from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n","from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import load_model, Model\n","import tensorflow.keras.backend as K\n","import tensorflow as tf\n","import numpy as np\n","\n","from faker import Faker\n","import random\n","from tqdm import tqdm\n","from babel.dates import format_date\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"metadata":{"id":"90uWSE0YnPwu","executionInfo":{"status":"ok","timestamp":1714037851137,"user_tz":-390,"elapsed":3,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["m = 10000\n","dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gp7bQrcangDI","executionInfo":{"status":"ok","timestamp":1714037865689,"user_tz":-390,"elapsed":1844,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}},"outputId":"ca2c94e0-a156-49ce-8d2a-2ab75adebb5a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10000/10000 [00:01<00:00, 6647.57it/s]\n"]}]},{"cell_type":"code","source":["dataset[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pa9gKSM4njMm","executionInfo":{"status":"ok","timestamp":1714037874532,"user_tz":-390,"elapsed":3,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}},"outputId":"d1c10101-2e81-442f-93aa-ca9dbbf9dd6d"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('18 aug 1992', '1992-08-18'),\n"," ('21.07.70', '1970-07-21'),\n"," ('10/27/14', '2014-10-27'),\n"," ('saturday march 22 1986', '1986-03-22'),\n"," ('thursday january 4 1990', '1990-01-04'),\n"," ('tuesday july 8 1980', '1980-07-08'),\n"," ('thursday september 28 2000', '2000-09-28'),\n"," ('13 oct 1978', '1978-10-13'),\n"," ('1 oct 1976', '1976-10-01'),\n"," ('wednesday july 7 1993', '1993-07-07')]"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["Tx = 30\n","Ty = 10\n","X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)\n","\n","print(\"X.shape:\", X.shape)\n","print(\"Y.shape:\", Y.shape)\n","print(\"Xoh.shape:\", Xoh.shape)\n","print(\"Yoh.shape:\", Yoh.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UnYcGmqInltX","executionInfo":{"status":"ok","timestamp":1714037884781,"user_tz":-390,"elapsed":1005,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}},"outputId":"3ba75579-2e4e-4a05-f19d-c37bf4534b58"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["X.shape: (10000, 30)\n","Y.shape: (10000, 10)\n","Xoh.shape: (10000, 30, 37)\n","Yoh.shape: (10000, 10, 11)\n"]}]},{"cell_type":"code","source":["index = 0\n","print(\"Source date:\", dataset[index][0])\n","print(\"Target date:\", dataset[index][1])\n","print()\n","print(\"Source after preprocessing (indices):\", X[index])\n","print(\"Target after preprocessing (indices):\", Y[index])\n","print()\n","print(\"Source after preprocessing (one-hot):\", Xoh[index])\n","print(\"Target after preprocessing (one-hot):\", Yoh[index])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1AecfkiOnoCW","executionInfo":{"status":"ok","timestamp":1714037896270,"user_tz":-390,"elapsed":3,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}},"outputId":"68352b43-d32b-425e-aaa0-6c084290fa4c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Source date: 18 aug 1992\n","Target date: 1992-08-18\n","\n","Source after preprocessing (indices): [ 4 11  0 13 31 19  0  4 12 12  5 36 36 36 36 36 36 36 36 36 36 36 36 36\n"," 36 36 36 36 36 36]\n","Target after preprocessing (indices): [ 2 10 10  3  0  1  9  0  2  9]\n","\n","Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [1. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 1.]\n"," [0. 0. 0. ... 0. 0. 1.]\n"," [0. 0. 0. ... 0. 0. 1.]]\n","Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n"," [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]]\n"]}]},{"cell_type":"code","source":["# Defined shared layers as global variables\n","repeator = RepeatVector(Tx)\n","concatenator = Concatenate(axis=-1)\n","densor1 = Dense(10, activation = \"tanh\")\n","densor2 = Dense(1, activation = \"relu\")\n","activator = Activation(softmax, name='attention_weights') # We are using a custom softmax(axis = 1) loaded in this notebook\n","dotor = Dot(axes = 1)"],"metadata":{"id":"dvXVZSLFnq-o","executionInfo":{"status":"ok","timestamp":1714037908212,"user_tz":-390,"elapsed":1,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["\n","def one_step_attention(a, s_prev):\n","    \"\"\"\n","    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n","    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n","\n","    Arguments:\n","    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n","    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n","\n","    Returns:\n","    context -- context vector, input of the next (post-attention) LSTM cell\n","    \"\"\"\n","\n","    ### START CODE HERE ###\n","    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n","    s_prev = repeator(s_prev)\n","    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n","    # For grading purposes, please list 'a' first and 's_prev' second, in this order.\n","    concat = concatenator([a,s_prev])\n","    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n","    e = densor1(concat)\n","    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n","    energies = densor2(e)\n","    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n","    alphas = activator(energies)\n","    # Use dotor together with \"alphas\" and \"a\", in this order, to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n","    context = dotor([alphas,a])\n","    ### END CODE HERE ###\n","\n","    return context"],"metadata":{"id":"KDGVisdYnt9Z","executionInfo":{"status":"ok","timestamp":1714037918173,"user_tz":-390,"elapsed":3,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["n_a = 32 # number of units for the pre-attention, bi-directional LSTM's hidden state 'a'\n","n_s = 64 # number of units for the post-attention LSTM's hidden state \"s\"\n","\n","# Please note, this is the post attention LSTM cell.\n","post_activation_LSTM_cell = LSTM(n_s, return_state = True) # Please do not modify this global variable.\n","output_layer = Dense(len(machine_vocab), activation=softmax)"],"metadata":{"id":"LNM6i57snwXW","executionInfo":{"status":"ok","timestamp":1714037928022,"user_tz":-390,"elapsed":2,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n","# GRADED FUNCTION: model\n","\n","def modelf(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n","    \"\"\"\n","    Arguments:\n","    Tx -- length of the input sequence\n","    Ty -- length of the output sequence\n","    n_a -- hidden state size of the Bi-LSTM\n","    n_s -- hidden state size of the post-attention LSTM\n","    human_vocab_size -- size of the python dictionary \"human_vocab\"\n","    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n","\n","    Returns:\n","    model -- Keras model instance\n","    \"\"\"\n","\n","    # Define the inputs of your model with a shape (Tx, human_vocab_size)\n","    # Define s0 (initial hidden state) and c0 (initial cell state)\n","    # for the decoder LSTM with shape (n_s,)\n","    X = Input(shape=(Tx, human_vocab_size))\n","    # initial hidden state\n","    s0 = Input(shape=(n_s,), name='s0')\n","    # initial cell state\n","    c0 = Input(shape=(n_s,), name='c0')\n","    # hidden state\n","    s = s0\n","    # cell state\n","    c = c0\n","\n","    # Initialize empty list of outputs\n","    outputs = []\n","\n","    ### START CODE HERE ###\n","\n","    # Step 1: Define your pre-attention Bi-LSTM. (≈ 1 line)\n","    a = Bidirectional(LSTM(units=n_a, return_sequences=True))(X)\n","\n","    # Step 2: Iterate for Ty steps\n","    for t in range(Ty):\n","\n","        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t (≈ 1 line)\n","        context = one_step_attention(a, s)\n","\n","        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector. (≈ 1 line)\n","        # Don't forget to pass: initial_state = [hidden state, cell state]\n","        # Remember: s = hidden state, c = cell state\n","        _, s, c = post_activation_LSTM_cell(inputs=context, initial_state=[s, c])\n","\n","        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM (≈ 1 line)\n","        out = output_layer(s)\n","\n","        # Step 2.D: Append \"out\" to the \"outputs\" list (≈ 1 line)\n","        outputs.append(out)\n","\n","    # Step 3: Create model instance taking three inputs and returning the list of outputs. (≈ 1 line)\n","    model = Model(inputs=[X,s0,c0], outputs=outputs)\n","\n","    ### END CODE HERE ###\n","\n","    return model"],"metadata":{"id":"BL-CotH3nyzC","executionInfo":{"status":"ok","timestamp":1714037941795,"user_tz":-390,"elapsed":340,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["model = modelf(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))"],"metadata":{"id":"wQXV8tUjn2FU","executionInfo":{"status":"ok","timestamp":1714037958824,"user_tz":-390,"elapsed":6385,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0o_XshBqn4u-","executionInfo":{"status":"ok","timestamp":1714037964708,"user_tz":-390,"elapsed":1146,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}},"outputId":"620a3350-4fc5-429d-8d37-c6cb9d49cc15"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_1 (InputLayer)        [(None, 30, 37)]             0         []                            \n","                                                                                                  \n"," s0 (InputLayer)             [(None, 64)]                 0         []                            \n","                                                                                                  \n"," bidirectional (Bidirection  (None, 30, 64)               17920     ['input_1[0][0]']             \n"," al)                                                                                              \n","                                                                                                  \n"," repeat_vector (RepeatVecto  (None, 30, 64)               0         ['s0[0][0]',                  \n"," r)                                                                  'lstm[0][1]',                \n","                                                                     'lstm[1][1]',                \n","                                                                     'lstm[2][1]',                \n","                                                                     'lstm[3][1]',                \n","                                                                     'lstm[4][1]',                \n","                                                                     'lstm[5][1]',                \n","                                                                     'lstm[6][1]',                \n","                                                                     'lstm[7][1]',                \n","                                                                     'lstm[8][1]']                \n","                                                                                                  \n"," concatenate (Concatenate)   (None, 30, 128)              0         ['bidirectional[0][0]',       \n","                                                                     'repeat_vector[0][0]',       \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'repeat_vector[1][0]',       \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'repeat_vector[2][0]',       \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'repeat_vector[3][0]',       \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'repeat_vector[4][0]',       \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'repeat_vector[5][0]',       \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'repeat_vector[6][0]',       \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'repeat_vector[7][0]',       \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'repeat_vector[8][0]',       \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'repeat_vector[9][0]']       \n","                                                                                                  \n"," dense (Dense)               (None, 30, 10)               1290      ['concatenate[0][0]',         \n","                                                                     'concatenate[1][0]',         \n","                                                                     'concatenate[2][0]',         \n","                                                                     'concatenate[3][0]',         \n","                                                                     'concatenate[4][0]',         \n","                                                                     'concatenate[5][0]',         \n","                                                                     'concatenate[6][0]',         \n","                                                                     'concatenate[7][0]',         \n","                                                                     'concatenate[8][0]',         \n","                                                                     'concatenate[9][0]']         \n","                                                                                                  \n"," dense_1 (Dense)             (None, 30, 1)                11        ['dense[0][0]',               \n","                                                                     'dense[1][0]',               \n","                                                                     'dense[2][0]',               \n","                                                                     'dense[3][0]',               \n","                                                                     'dense[4][0]',               \n","                                                                     'dense[5][0]',               \n","                                                                     'dense[6][0]',               \n","                                                                     'dense[7][0]',               \n","                                                                     'dense[8][0]',               \n","                                                                     'dense[9][0]']               \n","                                                                                                  \n"," attention_weights (Activat  (None, 30, 1)                0         ['dense_1[0][0]',             \n"," ion)                                                                'dense_1[1][0]',             \n","                                                                     'dense_1[2][0]',             \n","                                                                     'dense_1[3][0]',             \n","                                                                     'dense_1[4][0]',             \n","                                                                     'dense_1[5][0]',             \n","                                                                     'dense_1[6][0]',             \n","                                                                     'dense_1[7][0]',             \n","                                                                     'dense_1[8][0]',             \n","                                                                     'dense_1[9][0]']             \n","                                                                                                  \n"," dot (Dot)                   (None, 1, 64)                0         ['attention_weights[0][0]',   \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'attention_weights[1][0]',   \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'attention_weights[2][0]',   \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'attention_weights[3][0]',   \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'attention_weights[4][0]',   \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'attention_weights[5][0]',   \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'attention_weights[6][0]',   \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'attention_weights[7][0]',   \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'attention_weights[8][0]',   \n","                                                                     'bidirectional[0][0]',       \n","                                                                     'attention_weights[9][0]',   \n","                                                                     'bidirectional[0][0]']       \n","                                                                                                  \n"," c0 (InputLayer)             [(None, 64)]                 0         []                            \n","                                                                                                  \n"," lstm (LSTM)                 [(None, 64),                 33024     ['dot[0][0]',                 \n","                              (None, 64),                            's0[0][0]',                  \n","                              (None, 64)]                            'c0[0][0]',                  \n","                                                                     'dot[1][0]',                 \n","                                                                     'lstm[0][1]',                \n","                                                                     'lstm[0][2]',                \n","                                                                     'dot[2][0]',                 \n","                                                                     'lstm[1][1]',                \n","                                                                     'lstm[1][2]',                \n","                                                                     'dot[3][0]',                 \n","                                                                     'lstm[2][1]',                \n","                                                                     'lstm[2][2]',                \n","                                                                     'dot[4][0]',                 \n","                                                                     'lstm[3][1]',                \n","                                                                     'lstm[3][2]',                \n","                                                                     'dot[5][0]',                 \n","                                                                     'lstm[4][1]',                \n","                                                                     'lstm[4][2]',                \n","                                                                     'dot[6][0]',                 \n","                                                                     'lstm[5][1]',                \n","                                                                     'lstm[5][2]',                \n","                                                                     'dot[7][0]',                 \n","                                                                     'lstm[6][1]',                \n","                                                                     'lstm[6][2]',                \n","                                                                     'dot[8][0]',                 \n","                                                                     'lstm[7][1]',                \n","                                                                     'lstm[7][2]',                \n","                                                                     'dot[9][0]',                 \n","                                                                     'lstm[8][1]',                \n","                                                                     'lstm[8][2]']                \n","                                                                                                  \n"," dense_2 (Dense)             (None, 11)                   715       ['lstm[0][1]',                \n","                                                                     'lstm[1][1]',                \n","                                                                     'lstm[2][1]',                \n","                                                                     'lstm[3][1]',                \n","                                                                     'lstm[4][1]',                \n","                                                                     'lstm[5][1]',                \n","                                                                     'lstm[6][1]',                \n","                                                                     'lstm[7][1]',                \n","                                                                     'lstm[8][1]',                \n","                                                                     'lstm[9][1]']                \n","                                                                                                  \n","==================================================================================================\n","Total params: 52960 (206.88 KB)\n","Trainable params: 52960 (206.88 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["opt = Adam(learning_rate=0.005, beta_1=0.9, beta_2=0.999) # Adam(...)\n","model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])\n","\n","\n","\n"],"metadata":{"id":"WOBmdwiVn7e6","executionInfo":{"status":"ok","timestamp":1714038145921,"user_tz":-390,"elapsed":418,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["s0 = np.zeros((m, n_s))\n","c0 = np.zeros((m, n_s))\n","outputs = list(Yoh.swapaxes(0,1))"],"metadata":{"id":"TuLEC_Pdn-WD","executionInfo":{"status":"ok","timestamp":1714038160895,"user_tz":-390,"elapsed":4,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sxNjIxnQoroX","executionInfo":{"status":"ok","timestamp":1714038211308,"user_tz":-390,"elapsed":42304,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}},"outputId":"f7fe4065-5bb1-4423-a047-36dae96b2d4d"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["100/100 [==============================] - 33s 91ms/step - loss: 15.8770 - dense_2_loss: 1.0205 - dense_2_1_loss: 0.8433 - dense_2_2_loss: 1.6782 - dense_2_3_loss: 2.6316 - dense_2_4_loss: 0.7236 - dense_2_5_loss: 1.2308 - dense_2_6_loss: 2.6254 - dense_2_7_loss: 0.8726 - dense_2_8_loss: 1.6756 - dense_2_9_loss: 2.5754 - dense_2_accuracy: 0.5970 - dense_2_1_accuracy: 0.7631 - dense_2_2_accuracy: 0.3183 - dense_2_3_accuracy: 0.1056 - dense_2_4_accuracy: 0.8650 - dense_2_5_accuracy: 0.3896 - dense_2_6_accuracy: 0.0819 - dense_2_7_accuracy: 0.9061 - dense_2_8_accuracy: 0.2383 - dense_2_9_accuracy: 0.0890\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7ea20a292200>"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["model.load_weights('/content/drive/MyDrive/ML for coders/Deep Learning/Codes/C5/model.h5')"],"metadata":{"id":"zI2xg5f6otnY","executionInfo":{"status":"ok","timestamp":1714038977486,"user_tz":-390,"elapsed":361,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["EXAMPLES = ['3 May 1979', '5 April 09', '21th of August 2016', 'Tue 10 Jul 2007', 'Saturday May 9 2018', 'March 3 2001', 'March 3rd 2001', '1 March 2001']\n","s00 = np.zeros((1, n_s))\n","c00 = np.zeros((1, n_s))\n","for example in EXAMPLES:\n","    source = string_to_int(example, Tx, human_vocab)\n","    #print(source)\n","    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source))).swapaxes(0,1)\n","    source = np.swapaxes(source, 0, 1)\n","    source = np.expand_dims(source, axis=0)\n","    prediction = model.predict([source, s00, c00])\n","    prediction = np.argmax(prediction, axis = -1)\n","    output = [inv_machine_vocab[int(i)] for i in prediction]\n","    print(\"source:\", example)\n","    print(\"output:\", ''.join(output),\"\\n\")"],"metadata":{"id":"zoqUviH-ryx1","executionInfo":{"status":"ok","timestamp":1714038999418,"user_tz":-390,"elapsed":9301,"user":{"displayName":"s16162 Tun Tun Win","userId":"11564446739685333224"}},"outputId":"97883a11-89d1-4bd4-e81f-5d1a30438481","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 8s 8s/step\n","source: 3 May 1979\n","output: 1979-05-33 \n","\n","1/1 [==============================] - 0s 32ms/step\n","source: 5 April 09\n","output: 2009-04-05 \n","\n","1/1 [==============================] - ETA: 0s"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-26-dfa82594874d>:12: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n","  output = [inv_machine_vocab[int(i)] for i in prediction]\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1/1 [==============================] - 0s 36ms/step\n","source: 21th of August 2016\n","output: 2016-08-20 \n","\n","1/1 [==============================] - 0s 36ms/step\n","source: Tue 10 Jul 2007\n","output: 2007-07-10 \n","\n","1/1 [==============================] - 0s 32ms/step\n","source: Saturday May 9 2018\n","output: 2018-05-09 \n","\n","1/1 [==============================] - 0s 32ms/step\n","source: March 3 2001\n","output: 2001-03-03 \n","\n","1/1 [==============================] - 0s 32ms/step\n","source: March 3rd 2001\n","output: 2001-03-03 \n","\n","1/1 [==============================] - 0s 34ms/step\n","source: 1 March 2001\n","output: 2001-03-01 \n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"T73gI3tcr2I9"},"execution_count":null,"outputs":[]}]}